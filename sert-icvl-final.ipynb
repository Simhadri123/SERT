{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62e93e1",
   "metadata": {
    "papermill": {
     "duration": 0.004995,
     "end_time": "2025-10-15T11:34:14.298113",
     "exception": false,
     "start_time": "2025-10-15T11:34:14.293118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Unified Hyperspectral Denoising: Training and Evaluation\n",
    "\n",
    "This notebook consolidates the training and evaluation workflows of the provided scripts into a single, reproducible document. It inlines only local utilities (datasets, transforms, metrics) while importing model architectures from the `models` package by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aab70aa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T11:34:14.306804Z",
     "iopub.status.busy": "2025-10-15T11:34:14.306569Z",
     "iopub.status.idle": "2025-10-15T11:34:19.070436Z",
     "shell.execute_reply": "2025-10-15T11:34:19.069574Z"
    },
    "papermill": {
     "duration": 4.769502,
     "end_time": "2025-10-15T11:34:19.071633",
     "exception": false,
     "start_time": "2025-10-15T11:34:14.302131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\r\n",
      "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\r\n",
      "Collecting lmdb\r\n",
      "  Downloading lmdb-1.7.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (1.26.4)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (25.0)\r\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (3.20.3)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->tensorboardX) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->tensorboardX) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->tensorboardX) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->tensorboardX) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->tensorboardX) (2024.2.0)\r\n",
      "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading lmdb-1.7.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (295 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.1/295.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: lmdb, tensorboardX\r\n",
      "Successfully installed lmdb-1.7.5 tensorboardX-2.6.4\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboardX lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5661210",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T11:34:19.082192Z",
     "iopub.status.busy": "2025-10-15T11:34:19.081932Z",
     "iopub.status.idle": "2025-10-15T11:34:30.582963Z",
     "shell.execute_reply": "2025-10-15T11:34:30.582121Z"
    },
    "papermill": {
     "duration": 11.507373,
     "end_time": "2025-10-15T11:34:30.584074",
     "exception": false,
     "start_time": "2025-10-15T11:34:19.076701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Environment setup and imports\n",
    "import os, math, time, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "# Ensure offline behavior in network-restricted environments\n",
    "if os.path.exists('/kaggle') or 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "    os.environ.setdefault('WANDB_MODE', 'offline')\n",
    "\n",
    "# Reproducibility seed and device selection\n",
    "seed = 2018\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5272d412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T11:34:30.594633Z",
     "iopub.status.busy": "2025-10-15T11:34:30.593937Z",
     "iopub.status.idle": "2025-10-15T11:34:30.607540Z",
     "shell.execute_reply": "2025-10-15T11:34:30.606959Z"
    },
    "papermill": {
     "duration": 0.019672,
     "end_time": "2025-10-15T11:34:30.608577",
     "exception": false,
     "start_time": "2025-10-15T11:34:30.588905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "candidate_roots = [\n",
    "    os.path.abspath('.'),\n",
    "    '/kaggle/working',\n",
    "    '/kaggle/input',\n",
    "    # User-provided dataset hints\n",
    "    '/kaggle/input/hsi_denoising_all/other/default/5',\n",
    "]\n",
    "for root in candidate_roots:\n",
    "    if root and os.path.isdir(root):\n",
    "        if root not in sys.path:\n",
    "            sys.path.insert(0, root)\n",
    "        # Also scan one level deep under /kaggle/input for unpacked dataset dirs\n",
    "        try:\n",
    "            for name in os.listdir(root):\n",
    "                p = os.path.join(root, name)\n",
    "                if os.path.isdir(p) and (p not in sys.path):\n",
    "                    # If it looks like a repo folder with models/\n",
    "                    if os.path.exists(os.path.join(p, 'models', '__init__.py')):\n",
    "                        sys.path.insert(0, p)\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6537e2f",
   "metadata": {
    "papermill": {
     "duration": 0.00418,
     "end_time": "2025-10-15T11:34:30.617105",
     "exception": false,
     "start_time": "2025-10-15T11:34:30.612925",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13dd30f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T11:34:30.626649Z",
     "iopub.status.busy": "2025-10-15T11:34:30.626433Z",
     "iopub.status.idle": "2025-10-15T11:34:30.639313Z",
     "shell.execute_reply": "2025-10-15T11:34:30.638596Z"
    },
    "papermill": {
     "duration": 0.018968,
     "end_time": "2025-10-15T11:34:30.640431",
     "exception": false,
     "start_time": "2025-10-15T11:34:30.621463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Learning-rate utilities and parameter initialization\n",
    "import time, sys\n",
    "\n",
    "def adjust_learning_rate(optimizer, lr):\n",
    "    print(f'Adjust Learning Rate => {lr:.4e}')\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def display_learning_rate(optimizer):\n",
    "    lrs = []\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        lr = param_group['lr']\n",
    "        print('learning rate of group %d: %.4e' % (i, lr))\n",
    "        lrs.append(lr)\n",
    "    return lrs\n",
    "\n",
    "TOTAL_BAR_LENGTH = 50.0\n",
    "_last_time = time.time()\n",
    "_begin_time = _last_time\n",
    "\n",
    "def _format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds -= days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds -= hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds -= minutes*60\n",
    "    secondsf = int(seconds)\n",
    "    millis = int((seconds - secondsf)*1000)\n",
    "    f, i = '', 1\n",
    "    if days > 0 and i <= 2: f += str(days) + 'D'; i += 1\n",
    "    if hours > 0 and i <= 2: f += str(hours) + 'h'; i += 1\n",
    "    if minutes > 0 and i <= 2: f += str(minutes) + 'm'; i += 1\n",
    "    if secondsf > 0 and i <= 2: f += str(secondsf) + 's'; i += 1\n",
    "    if millis > 0 and i <= 2: f += str(millis) + 'ms'; i += 1\n",
    "    return f or '0ms'\n",
    "\n",
    "def progress_bar(current, total, msg=None):\n",
    "    global _last_time, _begin_time\n",
    "    if current == 0:\n",
    "        _begin_time = time.time()\n",
    "    cur_len = int(TOTAL_BAR_LENGTH * current / total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
    "    bar = '[' + '=' * cur_len + '>' + '.' * rest_len + ']'\n",
    "    cur_time = time.time()\n",
    "    step_time = cur_time - _last_time\n",
    "    _last_time = cur_time\n",
    "    tot_time = cur_time - _begin_time\n",
    "    progress_msg = f\"\\r{bar} {current+1}/{total} | Step: {_format_time(step_time)} | Tot: {_format_time(tot_time)}\"\n",
    "    if msg:\n",
    "        progress_msg += f\" | {msg}\"\n",
    "    sys.stdout.write(progress_msg)\n",
    "    if current >= total - 1:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def init_params(net, init_type='kn'):\n",
    "    print('use init scheme:', init_type)\n",
    "    if init_type != 'edsr':\n",
    "        for m in net.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Conv3d)):\n",
    "                if init_type == 'kn':\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if init_type == 'ku':\n",
    "                    nn.init.kaiming_uniform_(m.weight, mode='fan_out')\n",
    "                if init_type == 'xn':\n",
    "                    nn.init.xavier_normal_(m.weight)\n",
    "                if init_type == 'xu':\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, std=1e-3)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e636210",
   "metadata": {
    "papermill": {
     "duration": 0.004986,
     "end_time": "2025-10-15T11:34:30.649678",
     "exception": false,
     "start_time": "2025-10-15T11:34:30.644692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36957a48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T11:34:30.659150Z",
     "iopub.status.busy": "2025-10-15T11:34:30.658943Z",
     "iopub.status.idle": "2025-10-15T11:34:31.281082Z",
     "shell.execute_reply": "2025-10-15T11:34:31.280473Z"
    },
    "papermill": {
     "duration": 0.62874,
     "end_time": "2025-10-15T11:34:31.282544",
     "exception": false,
     "start_time": "2025-10-15T11:34:30.653804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data transforms and dataset wrappers\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from scipy.io import loadmat\n",
    "\n",
    "class AddNoise(object):\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma_ratio = sigma / 255.0\n",
    "    def __call__(self, img):\n",
    "        noise = np.random.randn(*img.shape) * self.sigma_ratio\n",
    "        return img + noise\n",
    "\n",
    "class AddNoiseBlindv1(object):\n",
    "    def __init__(self, min_sigma, max_sigma):\n",
    "        self.min_sigma = min_sigma\n",
    "        self.max_sigma = max_sigma\n",
    "    def __call__(self, img):\n",
    "        sigma = np.random.uniform(self.min_sigma, self.max_sigma) / 255\n",
    "        noise = np.random.randn(*img.shape) * sigma\n",
    "        return img + noise\n",
    "\n",
    "class HSI2Tensor(object):\n",
    "    def __init__(self, use_2dconv):\n",
    "        self.use_2dconv = use_2dconv\n",
    "    def __call__(self, hsi):\n",
    "        if self.use_2dconv:\n",
    "            img = torch.from_numpy(hsi)\n",
    "        else:\n",
    "            img = torch.from_numpy(hsi[None])\n",
    "        return img.float()\n",
    "\n",
    "class LoadMatHSI(object):\n",
    "    def __init__(self, input_key, gt_key, needsigma=False, transform=None):\n",
    "        self.gt_key = gt_key\n",
    "        self.input_key = input_key\n",
    "        self.transform = transform\n",
    "        self.needsigma = needsigma\n",
    "    def __call__(self, mat):\n",
    "        if self.transform:\n",
    "            _input = self.transform(mat[self.input_key][:].transpose((2,0,1)))\n",
    "            _gt = self.transform(mat[self.gt_key][:].transpose((2,0,1)))\n",
    "        else:\n",
    "            _input = mat[self.input_key][:].transpose((2,0,1))\n",
    "            _gt = mat[self.gt_key][:].transpose((2,0,1))\n",
    "        input_t = torch.from_numpy(_input).float()\n",
    "        gt_t = torch.from_numpy(_gt).float()\n",
    "        if self.needsigma:\n",
    "            sigma = torch.from_numpy(mat['sigma']).float()\n",
    "            return input_t, gt_t, sigma\n",
    "        return input_t, gt_t\n",
    "\n",
    "class MatDataFromFolder(Dataset):\n",
    "    def __init__(self, data_dir, suffix='.mat', fns=None, size=None):\n",
    "        super().__init__()\n",
    "        if fns is not None:\n",
    "            self.filenames = [os.path.join(data_dir, fn) for fn in fns]\n",
    "        else:\n",
    "            self.filenames = [os.path.join(data_dir, fn) for fn in os.listdir(data_dir) if fn.endswith(suffix)]\n",
    "        if size is not None and size <= len(self.filenames):\n",
    "            self.filenames = self.filenames[:size]\n",
    "    def __getitem__(self, index):\n",
    "        return loadmat(self.filenames[index])\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "class ImageTransformDataset(Dataset):\n",
    "    def __init__(self, dataset, transform, target_transform=None):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.length = len(dataset)\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.dataset[idx]\n",
    "        target = img.copy()\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target\n",
    "\n",
    "class TransformDatasetWrapper(Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.dataset[idx])\n",
    "\n",
    "class LMDBDataset(Dataset):\n",
    "    def __init__(self, db_path, repeat=1):\n",
    "        import lmdb\n",
    "        self.db_path = db_path\n",
    "        self.env = lmdb.open(db_path, max_readers=1, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            self.length = int(txn.stat()['entries'])\n",
    "        with open(os.path.join(db_path, 'meta_info.txt')) as fin:\n",
    "            line = fin.readlines()[0]\n",
    "            size = line.split('(')[1].split(')')[0]\n",
    "            h, w, c = [int(s) for s in size.split(',')]\n",
    "        self.channels = c; self.width = h; self.height = w\n",
    "        self.repeat = repeat\n",
    "    def __getitem__(self, index):\n",
    "        import numpy as _np\n",
    "        index = index % self.length\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            data = txn.get(f'{index:08d}'.encode('ascii'))\n",
    "        flat_x = _np.fromstring(data, dtype=_np.float32)\n",
    "        x = flat_x.reshape(self.channels, self.height, self.width)\n",
    "        return x\n",
    "    def __len__(self):\n",
    "        return self.length * self.repeat\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1322202",
   "metadata": {
    "papermill": {
     "duration": 0.004133,
     "end_time": "2025-10-15T11:34:31.291546",
     "exception": false,
     "start_time": "2025-10-15T11:34:31.287413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Metrics and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a8f094c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T11:34:31.301250Z",
     "iopub.status.busy": "2025-10-15T11:34:31.300932Z",
     "iopub.status.idle": "2025-10-15T11:34:32.109667Z",
     "shell.execute_reply": "2025-10-15T11:34:32.108941Z"
    },
    "papermill": {
     "duration": 0.815357,
     "end_time": "2025-10-15T11:34:32.111001",
     "exception": false,
     "start_time": "2025-10-15T11:34:31.295644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Band-wise metrics and structural similarity\n",
    "class Bandwise(object):\n",
    "    def __init__(self, index_fn):\n",
    "        self.index_fn = index_fn\n",
    "    def __call__(self, X, Y):\n",
    "        C = X.shape[-3]\n",
    "        bwindex = []\n",
    "        for ch in range(C):\n",
    "            x = torch.squeeze(X[..., ch, :, :].data).cpu().numpy()\n",
    "            y = torch.squeeze(Y[..., ch, :, :].data).cpu().numpy()\n",
    "            index = self.index_fn(x, y)\n",
    "            bwindex.append(index)\n",
    "        return bwindex\n",
    "\n",
    "# Prefer skimage.metrics when available\n",
    "try:\n",
    "    from skimage.metrics import structural_similarity as compare_ssim, peak_signal_noise_ratio as compare_psnr\n",
    "except ImportError:\n",
    "    from skimage.measure import compare_ssim, compare_psnr\n",
    "from functools import partial\n",
    "cal_bwpsnr = Bandwise(partial(compare_psnr, data_range=1))\n",
    "cal_bwssim = Bandwise(compare_ssim)\n",
    "\n",
    "def cal_sam(X, Y, eps=1e-8):\n",
    "    Xn = torch.squeeze(X.data).cpu().numpy()\n",
    "    Yn = torch.squeeze(Y.data).cpu().numpy()\n",
    "    tmp = (np.sum(Xn*Yn, axis=0) + eps) / (np.sqrt(np.sum(Xn**2, axis=0)) + eps) / (np.sqrt(np.sum(Yn**2, axis=0)) + eps)\n",
    "    return np.mean(np.real(np.arccos(tmp)))\n",
    "\n",
    "# SSIM loss and SAM loss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _fspecial_gauss_1d(size, sigma):\n",
    "    coords = torch.arange(size).to(dtype=torch.float)\n",
    "    coords -= size//2\n",
    "    g = torch.exp(-(coords**2) / (2*sigma**2))\n",
    "    g /= g.sum()\n",
    "    return g.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "def gaussian_filter(input, win):\n",
    "    N, C, H, W = input.shape\n",
    "    out = F.conv2d(input, win, stride=1, padding=0, groups=C)\n",
    "    out = out.transpose(2, 3).contiguous()\n",
    "    out = F.conv2d(out, win, stride=1, padding=0, groups=C)\n",
    "    return out.transpose(2, 3).contiguous()\n",
    "\n",
    "def _ssim(X, Y, win, data_range=255, size_average=True, full=False):\n",
    "    K1, K2 = 0.01, 0.03\n",
    "    channel = X.shape[1]\n",
    "    compensation = 1.0\n",
    "    C1 = (K1 * data_range)**2\n",
    "    C2 = (K2 * data_range)**2\n",
    "    concat_input = torch.cat([X, Y, X*X, Y*Y, X*Y], dim=1)\n",
    "    concat_win = win.repeat(5, 1, 1, 1).to(X.device, dtype=X.dtype)\n",
    "    concat_out = gaussian_filter(concat_input, concat_win)\n",
    "    mu1, mu2, sigma1_sq, sigma2_sq, sigma12 = (concat_out[:, idx*channel:(idx+1)*channel, :, :] for idx in range(5))\n",
    "    mu1_sq = mu1.pow(2); mu2_sq = mu2.pow(2); mu1_mu2 = mu1 * mu2\n",
    "    sigma1_sq = compensation * (sigma1_sq - mu1_sq)\n",
    "    sigma2_sq = compensation * (sigma2_sq - mu2_sq)\n",
    "    sigma12 = compensation * (sigma12 - mu1_mu2)\n",
    "    cs_map = (2 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) / (mu1_sq + mu2_sq + C1)) * cs_map\n",
    "    if size_average:\n",
    "        ssim_val = ssim_map.mean(); cs = cs_map.mean()\n",
    "    else:\n",
    "        ssim_val = ssim_map.mean(-1).mean(-1).mean(-1); cs = cs_map.mean(-1).mean(-1).mean(-1)\n",
    "    return (ssim_val, cs) if full else ssim_val\n",
    "\n",
    "def ssim(X, Y, win_size=11, win_sigma=1.5, win=None, data_range=255, size_average=True, full=False):\n",
    "    if len(X.shape) != 4: raise ValueError('Input images must 4-d tensor.')\n",
    "    if not X.type() == Y.type(): raise ValueError('Input images must have the same dtype.')\n",
    "    if not X.shape == Y.shape: raise ValueError('Input images must have the same dimensions.')\n",
    "    if not (win_size % 2 == 1): raise ValueError('Window size must be odd.')\n",
    "    if win is None:\n",
    "        win = _fspecial_gauss_1d(win_size, win_sigma).repeat(X.shape[1], 1, 1, 1)\n",
    "    ssim_val, cs = _ssim(X, Y, win=win, data_range=data_range, size_average=False, full=True)\n",
    "    if size_average:\n",
    "        ssim_val = ssim_val.mean(); cs = cs.mean()\n",
    "    return (ssim_val, cs) if full else ssim_val\n",
    "\n",
    "class SSIMLoss(torch.nn.Module):\n",
    "    def __init__(self, win_size=11, win_sigma=1.5, data_range=None, size_average=True, channel=3):\n",
    "        super().__init__()\n",
    "        self.win = _fspecial_gauss_1d(win_size, win_sigma).repeat(channel, 1, 1, 1)\n",
    "        self.size_average = size_average\n",
    "        self.data_range = data_range\n",
    "    def forward(self, X, Y):\n",
    "        if X.ndimension() == 5: X = X[:,0,...]\n",
    "        if Y.ndimension() == 5: Y = Y[:,0,...]\n",
    "        return 1 - ssim(X, Y, win=self.win, data_range=self.data_range, size_average=self.size_average)\n",
    "\n",
    "class SAMLoss(torch.nn.Module):\n",
    "    def __init__(self, size_average=False):\n",
    "        super().__init__()\n",
    "    def forward(self, img_base, img_out):\n",
    "        if img_base.ndimension() == 5: img_base = img_base[:,0,...]\n",
    "        if img_out.ndimension() == 5: img_out = img_out[:,0,...]\n",
    "        sum1 = torch.sum(img_base * img_out, 1)\n",
    "        sum2 = torch.sum(img_base * img_base, 1)\n",
    "        sum3 = torch.sum(img_out * img_out, 1)\n",
    "        t = (sum2 * sum3) ** 0.5\n",
    "        numlocal = torch.gt(t, 0)\n",
    "        num = torch.sum(numlocal)\n",
    "        t = sum1 / t\n",
    "        angle = torch.acos(t)\n",
    "        sumangle = torch.where(torch.isnan(angle), torch.full_like(angle, 0), angle).sum()\n",
    "        averangle = sumangle if num == 0 else sumangle / num\n",
    "        return averangle * 180 / 3.14159256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb3a1b",
   "metadata": {
    "papermill": {
     "duration": 0.004405,
     "end_time": "2025-10-15T11:34:32.120012",
     "exception": false,
     "start_time": "2025-10-15T11:34:32.115607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loss composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1cfa10e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T11:34:32.129823Z",
     "iopub.status.busy": "2025-10-15T11:34:32.129459Z",
     "iopub.status.idle": "2025-10-15T11:34:32.137489Z",
     "shell.execute_reply": "2025-10-15T11:34:32.136715Z"
    },
    "papermill": {
     "duration": 0.014429,
     "end_time": "2025-10-15T11:34:32.138671",
     "exception": false,
     "start_time": "2025-10-15T11:34:32.124242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combined and consistency losses\n",
    "class MultipleLoss(nn.Module):\n",
    "    def __init__(self, losses, weight=None):\n",
    "        super().__init__()\n",
    "        self.losses = nn.ModuleList(losses)\n",
    "        self.weight = weight or [1/len(self.losses)] * len(self.losses)\n",
    "    def forward(self, predict, target):\n",
    "        total_loss = 0\n",
    "        for w, loss in zip(self.weight, self.losses):\n",
    "            total_loss += loss(predict, target) * w\n",
    "        return total_loss\n",
    "    def extra_repr(self):\n",
    "        return f'weight={self.weight}'\n",
    "\n",
    "class L1Consist(nn.Module):\n",
    "    def __init__(self, losses, weight=None):\n",
    "        super().__init__()\n",
    "        self.loss1 = losses[0]\n",
    "        self.loss_cons = losses[1]\n",
    "        self.weight = weight or [1/len(losses)] * len(losses)\n",
    "    def forward(self, predict, target, inputs):\n",
    "        total_loss = 0\n",
    "        total_loss += self.loss1(predict, target) * self.weight[0]\n",
    "        # Contrastive consistency loss is not available in this repository.\n",
    "        raise RuntimeError('ContrastLoss is not available; use one of {\"l1\", \"l2\", \"smooth_l1\", \"ssim\", \"l2_ssim\", \"l2_sam\"}.')\n",
    "\n",
    "def build_criterion(loss_name):\n",
    "    if loss_name == 'l2': return nn.MSELoss()\n",
    "    if loss_name == 'l1': return nn.L1Loss()\n",
    "    if loss_name == 'smooth_l1': return nn.SmoothL1Loss()\n",
    "    if loss_name == 'ssim': return SSIMLoss(data_range=1, channel=31)\n",
    "    if loss_name == 'l2_ssim': return MultipleLoss([nn.MSELoss(), SSIMLoss(data_range=1, channel=31)], weight=[1, 2.5e-3])\n",
    "    if loss_name == 'l2_sam': return MultipleLoss([nn.MSELoss(), SAMLoss()], weight=[1, 1e-3])\n",
    "    if loss_name in ('cons', 'cons_l2'):\n",
    "        raise RuntimeError('Loss mode %s requires ContrastLoss which is not present in this repository.' % loss_name)\n",
    "    raise ValueError('Unknown loss_name: %s' % loss_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27608ab6",
   "metadata": {
    "papermill": {
     "duration": 0.004105,
     "end_time": "2025-10-15T11:34:32.147141",
     "exception": false,
     "start_time": "2025-10-15T11:34:32.143036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dd292f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T11:34:32.156712Z",
     "iopub.status.busy": "2025-10-15T11:34:32.156464Z",
     "iopub.status.idle": "2025-10-15T11:34:32.166358Z",
     "shell.execute_reply": "2025-10-15T11:34:32.165594Z"
    },
    "papermill": {
     "duration": 0.016192,
     "end_time": "2025-10-15T11:34:32.167600",
     "exception": false,
     "start_time": "2025-10-15T11:34:32.151408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU IDs: [0]\n",
      "eval_during_train = False\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "dataroot = '/kaggle/input/icvl64-31-hs-v2/content/ICVL64_31.db'  # LMDB path for training patches\n",
    "val_data_dir = '/kaggle/input/icvl-test-512'  # directory of .mat files for validation\n",
    "test_dir = '/kaggle/input/icvl-test-512'  # directory of .mat files for testing\n",
    "# Validation will use val_data_dir directly (no separate eval_dir)\n",
    "save_dir = 'checkpoints'\n",
    "checkpoint_dir = os.path.join(save_dir, 'model')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Training hyperparameters\n",
    "batch_size = 16\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.0\n",
    "# When resuming, num_epochs indicates how many ADDITIONAL epochs to run beyond the checkpoint epoch count.\n",
    "# Example: if the checkpoint is at epoch 10 and num_epochs=1, training will run for epoch 11 only.\n",
    "num_epochs = 70\n",
    "clip = 1e6\n",
    "num_workers = 0  # DataLoader workers\n",
    "no_cuda = False\n",
    "no_log = True\n",
    "\n",
    "# Control flags\n",
    "chop = False\n",
    "resume = True\n",
    "resumePath = '/kaggle/input/sert-icvl-temp/model_latest.pth'  # Path to a checkpoint with keys {'net', 'optimizer', 'epoch'}; 'epoch' is treated as completed epochs\n",
    "\n",
    "# Validation cadence\n",
    "val_every = 5  # run validate_epoch every N epochs; saves best+latest when validation runs\n",
    "\n",
    "# Model and loss\n",
    "model_class_or_name = 'sert_base'  # Name of model in models package (callable, e.g., 'sert_base')\n",
    "init_scheme = 'kn'  # ['kn','ku','xn','xu','edsr']\n",
    "loss_name = 'l2'  # ['l1','l2','smooth_l1','ssim','l2_ssim','l2_sam']\n",
    "use_2dconv = True  # will be auto-detected from model if attribute exists\n",
    "\n",
    "# GPU configuration (mirrors hsi_setup --gpu-ids)\n",
    "# Provide a comma-separated string like '0' or '0,1' to use multiple GPUs\n",
    "_gpu_ids_str = '0'\n",
    "def _parse_gpu_ids(args_str):\n",
    "    parts = [s.strip() for s in args_str.split(',') if s.strip() != '']\n",
    "    parsed = []\n",
    "    for p in parts:\n",
    "        try:\n",
    "            v = int(p)\n",
    "            if v >= 0:\n",
    "                parsed.append(v)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return parsed\n",
    "\n",
    "gpu_ids = _parse_gpu_ids(_gpu_ids_str)\n",
    "print('GPU IDs:', gpu_ids)\n",
    "\n",
    "# Noise levels for simulated testing\n",
    "sigma = None\n",
    "sigma_test = 10\n",
    "\n",
    "# Scheduler (original script decays LR at epoch==50)\n",
    "step_decay_epoch = 250\n",
    "gamma = 0.1\n",
    "\n",
    "# Training-time mode toggle\n",
    "# In the original scripts, the forward pass during training runs with net.eval() set\n",
    "# because Engine.__step() unconditionally calls self.net.eval().\n",
    "# Set this to True to match script behavior; set to False for conventional train-mode forwards.\n",
    "eval_during_train = False\n",
    "print('eval_during_train =', eval_during_train)\n",
    "\n",
    "# Meta-learning params (defined in options; not used in these loops)\n",
    "update_lr = 0.5e-4\n",
    "meta_lr = 0.5e-4\n",
    "n_way = 1\n",
    "k_spt = 2\n",
    "k_qry = 5\n",
    "task_num = 16\n",
    "update_step = 5\n",
    "update_step_test = 10\n",
    "\n",
    "# Reproducibility\n",
    "seed = 2018\n",
    "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "# Device selection\n",
    "device = 'cuda' if (not no_cuda and torch.cuda.is_available()) else 'cpu'\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0301b8",
   "metadata": {
    "papermill": {
     "duration": 0.004032,
     "end_time": "2025-10-15T11:34:32.175940",
     "exception": false,
     "start_time": "2025-10-15T11:34:32.171908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model initialization and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac3d9cf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T11:34:32.185605Z",
     "iopub.status.busy": "2025-10-15T11:34:32.185194Z",
     "iopub.status.idle": "2025-10-15T11:34:37.590710Z",
     "shell.execute_reply": "2025-10-15T11:34:37.589778Z"
    },
    "papermill": {
     "duration": 5.41169,
     "end_time": "2025-10-15T11:34:37.592051",
     "exception": false,
     "start_time": "2025-10-15T11:34:32.180361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating model 'sert_base'\n",
      "3\n",
      "use init scheme: kn\n",
      "use_2dconv = True\n",
      "Criterion: MSELoss()\n",
      "Number of parameters: 1.905319 M\n",
      "==> Resuming from checkpoint: /kaggle/input/sert-icvl-temp/model_latest.pth\n",
      "Resumed from completed epoch: 200\n"
     ]
    }
   ],
   "source": [
    "import models  # import from local models/ package\n",
    "\n",
    "print(f\"=> creating model '{model_class_or_name}'\")\n",
    "net = getattr(models, model_class_or_name)()\n",
    "# Initialize params as in helper\n",
    "init_params(net, init_type=init_scheme)\n",
    "# Auto-detect 2D/3D conv usage if attribute exists\n",
    "if hasattr(net, 'use_2dconv'):\n",
    "    use_2dconv = bool(getattr(net, 'use_2dconv'))\n",
    "print('use_2dconv =', use_2dconv)\n",
    "\n",
    "# Device and optional DataParallel based on gpu_ids\n",
    "if device == 'cuda' and isinstance(gpu_ids, list) and len(gpu_ids) > 1:\n",
    "    net = nn.DataParallel(net.cuda(), device_ids=gpu_ids, output_device=gpu_ids[0])\n",
    "else:\n",
    "    net = net.to(device)\n",
    "\n",
    "criterion = build_criterion(loss_name).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay, amsgrad=False)\n",
    "base_lr = learning_rate\n",
    "print('Criterion:', criterion)\n",
    "print('Number of parameters:', sum(p.numel() for p in net.parameters())/1e6, 'M')\n",
    "\n",
    "# Resume support: load checkpoint and lift epoch/optimizer state if requested\n",
    "start_epoch_completed = 0\n",
    "if resume and resumePath and os.path.exists(resumePath):\n",
    "    print('==> Resuming from checkpoint:', resumePath)\n",
    "    ckpt = torch.load(resumePath, map_location=device)\n",
    "    state = ckpt.get('net', ckpt)\n",
    "    try:\n",
    "        net.load_state_dict(state, strict=False)\n",
    "    except Exception as e:\n",
    "        print('Warning: non-strict load of state_dict due to mismatch:', e)\n",
    "    if 'optimizer' in ckpt:\n",
    "        try:\n",
    "            optimizer.load_state_dict(ckpt['optimizer'])\n",
    "        except Exception as e:\n",
    "            print('Warning: could not load optimizer state:', e)\n",
    "    start_epoch_completed = int(ckpt.get('epoch', 0))\n",
    "    print(f'Resumed from completed epoch: {start_epoch_completed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d02e3",
   "metadata": {
    "papermill": {
     "duration": 0.004624,
     "end_time": "2025-10-15T11:34:37.601524",
     "exception": false,
     "start_time": "2025-10-15T11:34:37.596900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1312f2aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T11:34:37.612233Z",
     "iopub.status.busy": "2025-10-15T11:34:37.611833Z",
     "iopub.status.idle": "2025-10-15T11:34:37.646582Z",
     "shell.execute_reply": "2025-10-15T11:34:37.645795Z"
    },
    "papermill": {
     "duration": 0.04177,
     "end_time": "2025-10-15T11:34:37.647817",
     "exception": false,
     "start_time": "2025-10-15T11:34:37.606047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 3200\n",
      "Validation samples: 50\n",
      "Test samples: 50\n"
     ]
    }
   ],
   "source": [
    "# Training dataset: LMDB patches with blind Gaussian noise\n",
    "try:\n",
    "    icvl_dataset = LMDBDataset(dataroot)\n",
    "    train_transform = Compose([\n",
    "        AddNoiseBlindv1(10, 70),\n",
    "        HSI2Tensor(use_2dconv)\n",
    "    ])\n",
    "    target_transform = HSI2Tensor(use_2dconv)\n",
    "    train_dataset = ImageTransformDataset(icvl_dataset, train_transform, target_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=(device=='cuda'), worker_init_fn=worker_init_fn)\n",
    "    print('Training samples:', len(train_dataset))\n",
    "except Exception as e:\n",
    "    print('Warning: LMDBDataset unavailable; training may be skipped. Error:', e)\n",
    "    train_loader = None\n",
    "\n",
    "# Validation dataset: .mat files with simulated Gaussian noise on inputs only\n",
    "val_mat_dataset = MatDataFromFolder(val_data_dir)\n",
    "\n",
    "\n",
    "\n",
    "if not use_2dconv:\n",
    "    val_mat_transform = Compose([\n",
    "        LoadMatHSI(input_key='input', gt_key='gt', transform=lambda x: x[...][None], needsigma=False),\n",
    "        # Add noise only to the input, keep gt clean\n",
    "        (lambda x: (\n",
    "            AddNoise(sigma_test)(x[0].numpy()) if isinstance(x[0], torch.Tensor) else AddNoise(sigma_val)(x[0]),\n",
    "            x[1]\n",
    "        ))\n",
    "    ])\n",
    "else:\n",
    "    val_mat_transform = Compose([\n",
    "        LoadMatHSI(input_key='input', gt_key='gt', needsigma=False),\n",
    "        (lambda x: (\n",
    "            AddNoise(sigma_test)(x[0].numpy()) if isinstance(x[0], torch.Tensor) else AddNoise(sigma_val)(x[0]),\n",
    "            x[1]\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "def _apply_val_transform(sample):\n",
    "    inp, gt = val_mat_transform(sample)\n",
    "    # Ensure both are torch tensors\n",
    "    if isinstance(inp, np.ndarray): inp = torch.from_numpy(inp).float()\n",
    "    if isinstance(gt, np.ndarray): gt = torch.from_numpy(gt).float()\n",
    "    return inp, gt\n",
    "\n",
    "val_dataset_wrapped = TransformDatasetWrapper(val_mat_dataset, _apply_val_transform)\n",
    "val_loader = DataLoader(val_dataset_wrapped, batch_size=1, shuffle=False, num_workers=1, pin_memory=(device == 'cuda'))\n",
    "print('Validation samples:', len(val_dataset_wrapped))\n",
    "\n",
    "\n",
    "# Test dataset: .mat files with simulated noise on inputs only (uses test_dir)\n",
    "mat_dataset = MatDataFromFolder(test_dir)\n",
    "if not use_2dconv:\n",
    "    mat_transform = Compose([\n",
    "        LoadMatHSI(input_key='input', gt_key='gt', transform=lambda x: x[...][None], needsigma=False),\n",
    "        # Add noise only to input, keep gt clean\n",
    "        (lambda x: (AddNoise(sigma_test)(x[0].numpy()) if isinstance(x[0], torch.Tensor) else AddNoise(sigma_test)(x[0]), x[1]))\n",
    "    ])\n",
    "else:\n",
    "    mat_transform = Compose([\n",
    "        LoadMatHSI(input_key='input', gt_key='gt', needsigma=False),\n",
    "        (lambda x: (AddNoise(sigma_test)(x[0].numpy()) if isinstance(x[0], torch.Tensor) else AddNoise(sigma_test)(x[0]), x[1]))\n",
    "    ])\n",
    "\n",
    "def _apply_mat_transform(sample):\n",
    "    inp, gt = mat_transform(sample)\n",
    "    # mat_transform returns numpy arrays when noise added; ensure tensors\n",
    "    if isinstance(inp, np.ndarray): inp = torch.from_numpy(inp).float()\n",
    "    if isinstance(gt, np.ndarray): gt = torch.from_numpy(gt).float()\n",
    "    return inp, gt\n",
    "\n",
    "mat_dataset_wrapped = TransformDatasetWrapper(mat_dataset, _apply_mat_transform)\n",
    "mat_loader = DataLoader(mat_dataset_wrapped, batch_size=1, shuffle=False, num_workers=1, pin_memory=(device=='cuda'))\n",
    "print('Test samples:', len(mat_dataset_wrapped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3e316d",
   "metadata": {
    "papermill": {
     "duration": 0.004228,
     "end_time": "2025-10-15T11:34:37.656722",
     "exception": false,
     "start_time": "2025-10-15T11:34:37.652494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d4083fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T11:34:37.667762Z",
     "iopub.status.busy": "2025-10-15T11:34:37.667468Z",
     "iopub.status.idle": "2025-10-15T11:34:37.681511Z",
     "shell.execute_reply": "2025-10-15T11:34:37.680764Z"
    },
    "papermill": {
     "duration": 0.02094,
     "end_time": "2025-10-15T11:34:37.682711",
     "exception": false,
     "start_time": "2025-10-15T11:34:37.661771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validation and testing routines\n",
    "@torch.no_grad()\n",
    "def _forward_step(net, inputs, targets):\n",
    "    return net(inputs.float())\n",
    "\n",
    "\n",
    "def validate_epoch(net, loader, name='val'):\n",
    "    # Evaluate in inference mode for metric stability\n",
    "    was_training = net.training\n",
    "    net.eval()\n",
    "    validate_loss = 0.0\n",
    "    total_psnr = 0.0\n",
    "    total_sam = 0.0\n",
    "    RMSE, SSIM, SAM, ERGAS, PSNR = [], [], [], [], []\n",
    "    for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = _forward_step(net, inputs, targets)\n",
    "        loss_data = criterion(outputs, targets).item()\n",
    "        psnr = np.mean(cal_bwpsnr(outputs, targets))\n",
    "        sam = cal_sam(outputs, targets)\n",
    "        validate_loss += loss_data\n",
    "        total_psnr += psnr\n",
    "        total_sam += sam\n",
    "        avg_loss = validate_loss / (batch_idx+1)\n",
    "        avg_psnr = total_psnr / (batch_idx+1)\n",
    "        progress_bar(batch_idx, len(loader), 'Loss: %.4e | PSNR: %.4f | AVGPSNR: %.4f' % (avg_loss, psnr, avg_psnr))\n",
    "        # Scalar metrics\n",
    "        psnr_bands = []\n",
    "        h, w = inputs.shape[-2:]\n",
    "        band = inputs.shape[-3]\n",
    "        result = outputs.squeeze().detach().cpu().numpy()\n",
    "        img = targets.squeeze().detach().cpu().numpy()\n",
    "        for k in range(band):\n",
    "            psnr_bands.append(10*np.log10((h*w)/np.sum((result[k]-img[k])**2)))\n",
    "        PSNR.append(np.mean(psnr_bands))\n",
    "        mse = np.sum((result-img)**2) / (band*h*w) * 255*255\n",
    "        RMSE.append(np.sqrt(mse))\n",
    "        ssim_vals = []\n",
    "        k1, k2 = 0.01, 0.03\n",
    "        for k in range(band):\n",
    "            cov = np.cov(result[k].reshape(h*w), img[k].reshape(h*w))[0,1]\n",
    "            ssim_vals.append((2*np.mean(result[k])*np.mean(img[k])+k1**2) * (2*cov + k2**2) / (np.mean(result[k])**2+np.mean(img[k])**2+k1**2) / (np.var(result[k])+np.var(img[k])+k2**2))\n",
    "        SSIM.append(np.mean(ssim_vals))\n",
    "        temp = (np.sum(result*img, 0) + np.spacing(1)) /(np.sqrt(np.sum(result**2, 0) + np.spacing(1))) /(np.sqrt(np.sum(img**2, 0) + np.spacing(1)))\n",
    "        SAM.append(np.mean(np.arccos(temp))*180/np.pi)\n",
    "        ergas = 0.0\n",
    "        for k in range(band):\n",
    "            ergas += np.mean((img[k]-result[k])**2)/np.mean(img[k])**2\n",
    "        ERGAS.append(100*np.sqrt(ergas/band))\n",
    "    final = {\n",
    "        'psnr': float(np.mean(PSNR)) if len(PSNR) else None,\n",
    "        'rmse': float(np.mean(RMSE)) if len(RMSE) else None,\n",
    "        'ssim': float(np.mean(SSIM)) if len(SSIM) else None,\n",
    "        'sam': float(np.mean(SAM)) if len(SAM) else None,\n",
    "        'ergas': float(np.mean(ERGAS)) if len(ERGAS) else None,\n",
    "        'loss': float(validate_loss/len(loader)) if len(loader) else None\n",
    "    }\n",
    "    print('\\n' + '='*60)\n",
    "    print(f' {name.upper()} SUMMARY')\n",
    "    print('='*60)\n",
    "    if final['psnr'] is not None:\n",
    "        print(f\"PSNR: {final['psnr']:.4f} dB | SSIM: {final['ssim']:.4f} | SAM: {final['sam']:.4f}° | RMSE: {final['rmse']:.4f} | ERGAS: {final['ergas']:.4f}\")\n",
    "    print('='*60)\n",
    "    if was_training and not eval_during_train:\n",
    "        net.train()\n",
    "    return final\n",
    "\n",
    "\n",
    "def test_eval(net, loader):\n",
    "    return validate_epoch(net, loader, name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f62063d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T11:34:37.693340Z",
     "iopub.status.busy": "2025-10-15T11:34:37.692540Z",
     "iopub.status.idle": "2025-10-15T11:34:37.697664Z",
     "shell.execute_reply": "2025-10-15T11:34:37.696860Z"
    },
    "papermill": {
     "duration": 0.01164,
     "end_time": "2025-10-15T11:34:37.698971",
     "exception": false,
     "start_time": "2025-10-15T11:34:37.687331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Training loop with resume-aware epoch counting\n",
    "# metrics_log = []\n",
    "# start_time = time.time()\n",
    "# # start_epoch_completed is set in model setup (0 if not resuming)\n",
    "# current_epoch_completed = start_epoch_completed\n",
    "# best_psnr = -float('inf')\n",
    "# best_ckpt_path = os.path.join(checkpoint_dir, 'model_best.pth')\n",
    "\n",
    "# if 'train_loader' not in globals():\n",
    "#     train_loader = None\n",
    "\n",
    "# if train_loader is None:\n",
    "#     print('Training loader is unavailable. Skipping training.')\n",
    "# else:\n",
    "#     # Run exactly num_epochs additional epochs beyond the checkpoint's completed epoch count\n",
    "#     for add_epoch_idx in range(num_epochs):\n",
    "#         # Epoch index to display and save as completed after this loop\n",
    "#         epoch_to_run = current_epoch_completed + 1\n",
    "#         # Match script behavior if eval_during_train is true\n",
    "#         if eval_during_train:\n",
    "#             net.eval()\n",
    "#         else:\n",
    "#             net.train()\n",
    "#         train_loss_sum = 0.0\n",
    "#         train_psnr_sum = 0.0\n",
    "#         for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = net(inputs.float())\n",
    "#             loss = criterion(outputs, targets)\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "#             optimizer.step()\n",
    "#             train_loss_sum += loss.item()\n",
    "#             psnr = np.mean(cal_bwpsnr(outputs, targets))\n",
    "#             train_psnr_sum += psnr\n",
    "#             avg_loss = train_loss_sum / (batch_idx+1)\n",
    "#             avg_psnr = train_psnr_sum / (batch_idx+1)\n",
    "#             progress_bar(batch_idx, len(train_loader), 'Epoch: %d | AvgLoss: %.4e | Loss: %.4e | PSNR: %4e' % (epoch_to_run, avg_loss, loss.item(), psnr))\n",
    "#         # Learning-rate step decay based on absolute epoch number\n",
    "#         if epoch_to_run == step_decay_epoch:\n",
    "#             adjust_learning_rate(optimizer, base_lr * gamma)\n",
    "#         # Save latest checkpoint after the epoch is completed\n",
    "#         model_latest_path = os.path.join(checkpoint_dir, 'model_latest.pth')\n",
    "#         torch.save({'net': net.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch_to_run}, model_latest_path)\n",
    "#         # Periodic validation\n",
    "#         if (epoch_to_run) % max(1, val_every) == 0 and 'val_loader' in globals() and val_loader is not None:\n",
    "#             val_stats = validate_epoch(net, val_loader, name='val')\n",
    "#             # Save per-validation checkpoint tagged with absolute epoch number\n",
    "#             model_val_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch_to_run}.pth')\n",
    "#             torch.save({'net': net.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch_to_run, 'val': val_stats}, model_val_path)\n",
    "#             # Track best by PSNR\n",
    "#             current_psnr = val_stats.get('psnr') or -float('inf')\n",
    "#             if current_psnr > best_psnr:\n",
    "#                 best_psnr = current_psnr\n",
    "#                 torch.save({'net': net.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch_to_run, 'val': val_stats}, best_ckpt_path)\n",
    "#                 print(f'New best PSNR {best_psnr:.4f} dB at epoch {epoch_to_run}. Saved to {best_ckpt_path}')\n",
    "#             metrics_log.append({\n",
    "#                 'epoch': epoch_to_run,\n",
    "#                 'train_loss': float(avg_loss),\n",
    "#                 'train_psnr': float(avg_psnr),\n",
    "#                 'val_loss': val_stats.get('loss'),\n",
    "#                 'psnr': val_stats.get('psnr'),\n",
    "#                 'ssim': val_stats.get('ssim'),\n",
    "#                 'sam': val_stats.get('sam'),\n",
    "#                 'rmse': val_stats.get('rmse'),\n",
    "#                 'ergas': val_stats.get('ergas')\n",
    "#             })\n",
    "#         else:\n",
    "#             metrics_log.append({\n",
    "#                 'epoch': epoch_to_run,\n",
    "#                 'train_loss': float(avg_loss),\n",
    "#                 'train_psnr': float(avg_psnr)\n",
    "#             })\n",
    "#         # Increment completed epoch count\n",
    "#         current_epoch_completed = epoch_to_run\n",
    "\n",
    "# elapsed = time.time() - start_time\n",
    "# print('Total training time (s):', elapsed)\n",
    "\n",
    "# # Final evaluation on the test set (noisy inputs)\n",
    "# test_stats = test_eval(net, mat_loader) if 'mat_loader' in globals() else {}\n",
    "# print('Test stats:', test_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63663eb4",
   "metadata": {
    "papermill": {
     "duration": 0.004701,
     "end_time": "2025-10-15T11:34:37.708283",
     "exception": false,
     "start_time": "2025-10-15T11:34:37.703582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Logging and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7a2fb37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T11:34:37.718565Z",
     "iopub.status.busy": "2025-10-15T11:34:37.718338Z",
     "iopub.status.idle": "2025-10-15T11:34:37.721874Z",
     "shell.execute_reply": "2025-10-15T11:34:37.721198Z"
    },
    "papermill": {
     "duration": 0.010074,
     "end_time": "2025-10-15T11:34:37.723024",
     "exception": false,
     "start_time": "2025-10-15T11:34:37.712950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from IPython.display import display\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "\n",
    "# # Persist metrics to CSV\n",
    "# df = pd.DataFrame(metrics_log) if len(metrics_log) else pd.DataFrame([test_stats])\n",
    "# csv_path = os.path.join(save_dir, 'training_metrics.csv')\n",
    "# df.to_csv(csv_path, index=False)\n",
    "# print('Saved metrics to', csv_path)\n",
    "# display(df.head())\n",
    "\n",
    "# # Plot only Train Loss\n",
    "# if 'epoch' in df.columns and 'train_loss' in df.columns:\n",
    "#     plt.figure(figsize=(8,5))\n",
    "#     plt.plot(df['epoch'], df['train_loss'], label='Train Loss', color='tab:blue')\n",
    "#     plt.title('Training Loss over Epochs')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True, linestyle='--', alpha=0.6)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fcdeb5",
   "metadata": {
    "papermill": {
     "duration": 0.004527,
     "end_time": "2025-10-15T11:34:37.732461",
     "exception": false,
     "start_time": "2025-10-15T11:34:37.727934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Simulated test\n",
    "\n",
    "constructing the model, loading a checkpoint, creating a .mat DataLoader with Gaussian noise on inputs (sigma_test), and reporting metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a1d425c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T11:34:37.742958Z",
     "iopub.status.busy": "2025-10-15T11:34:37.742703Z",
     "iopub.status.idle": "2025-10-15T11:43:37.744810Z",
     "shell.execute_reply": "2025-10-15T11:43:37.743918Z"
    },
    "papermill": {
     "duration": 540.008876,
     "end_time": "2025-10-15T11:43:37.746124",
     "exception": false,
     "start_time": "2025-10-15T11:34:37.737248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model sert_base\n",
      "3\n",
      "Loading checkpoint from /kaggle/input/sert-icvl-checkpoint/icvl_gaussian.pth\n",
      "use_2dconv = True\n",
      "Testing with noise sigma = 10\n",
      "[=================================================>] 50/50 | Step: 2s108ms | Tot: 1m34s | Loss: 1.9800e-05 | PSNR: 49.8268 | AVGPSNR: 47.7196\n",
      "\n",
      "============================================================\n",
      " TEST_SIGMA10 SUMMARY\n",
      "============================================================\n",
      "PSNR: 47.7196 dB | SSIM: 0.9988 | SAM: 1.3612° | RMSE: 1.1227 | ERGAS: 3.8066\n",
      "============================================================\n",
      "Testing with noise sigma = 30\n",
      "[=================================================>] 50/50 | Step: 2s184ms | Tot: 1m47s | Loss: 5.3823e-05 | PSNR: 45.0095 | AVGPSNR: 43.5608\n",
      "\n",
      "============================================================\n",
      " TEST_SIGMA30 SUMMARY\n",
      "============================================================\n",
      "PSNR: 43.5608 dB | SSIM: 0.9969 | SAM: 1.7687° | RMSE: 1.8509 | ERGAS: 5.8625\n",
      "============================================================\n",
      "Testing with noise sigma = 50\n",
      "[=================================================>] 50/50 | Step: 2s178ms | Tot: 1m47s | Loss: 9.2037e-05 | PSNR: 42.6882 | AVGPSNR: 41.3323\n",
      "\n",
      "============================================================\n",
      " TEST_SIGMA50 SUMMARY\n",
      "============================================================\n",
      "PSNR: 41.3323 dB | SSIM: 0.9949 | SAM: 2.0575° | RMSE: 2.4169 | ERGAS: 7.3456\n",
      "============================================================\n",
      "Testing with noise sigma = 70\n",
      "[=================================================>] 50/50 | Step: 2s184ms | Tot: 1m47s | Loss: 1.3219e-04 | PSNR: 40.9428 | AVGPSNR: 39.8217\n",
      "\n",
      "============================================================\n",
      " TEST_SIGMA70 SUMMARY\n",
      "============================================================\n",
      "PSNR: 39.8217 dB | SSIM: 0.9928 | SAM: 2.2937° | RMSE: 2.8941 | ERGAS: 8.5476\n",
      "============================================================\n",
      "Testing with noise sigma = blind\n",
      "[=================================================>] 50/50 | Step: 2s221ms | Tot: 1m47s | Loss: 7.6588e-05 | PSNR: 45.2001 | AVGPSNR: 42.9495\n",
      "\n",
      "============================================================\n",
      " TEST_SIGMABLIND SUMMARY\n",
      "============================================================\n",
      "PSNR: 42.9495 dB | SSIM: 0.9961 | SAM: 1.8645° | RMSE: 2.1019 | ERGAS: 6.3266\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_62657\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_62657_level0_col0\" class=\"col_heading level0 col0\" >run</th>\n",
       "      <th id=\"T_62657_level0_col1\" class=\"col_heading level0 col1\" >psnr</th>\n",
       "      <th id=\"T_62657_level0_col2\" class=\"col_heading level0 col2\" >ssim</th>\n",
       "      <th id=\"T_62657_level0_col3\" class=\"col_heading level0 col3\" >sam</th>\n",
       "      <th id=\"T_62657_level0_col4\" class=\"col_heading level0 col4\" >rmse</th>\n",
       "      <th id=\"T_62657_level0_col5\" class=\"col_heading level0 col5\" >ergas</th>\n",
       "      <th id=\"T_62657_level0_col6\" class=\"col_heading level0 col6\" >loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_62657_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_62657_row0_col0\" class=\"data row0 col0\" >sigma_10</td>\n",
       "      <td id=\"T_62657_row0_col1\" class=\"data row0 col1\" >47.7196</td>\n",
       "      <td id=\"T_62657_row0_col2\" class=\"data row0 col2\" >0.9988</td>\n",
       "      <td id=\"T_62657_row0_col3\" class=\"data row0 col3\" >1.3612</td>\n",
       "      <td id=\"T_62657_row0_col4\" class=\"data row0 col4\" >1.1227</td>\n",
       "      <td id=\"T_62657_row0_col5\" class=\"data row0 col5\" >3.8066</td>\n",
       "      <td id=\"T_62657_row0_col6\" class=\"data row0 col6\" >1.9800e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62657_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_62657_row1_col0\" class=\"data row1 col0\" >sigma_30</td>\n",
       "      <td id=\"T_62657_row1_col1\" class=\"data row1 col1\" >43.5608</td>\n",
       "      <td id=\"T_62657_row1_col2\" class=\"data row1 col2\" >0.9969</td>\n",
       "      <td id=\"T_62657_row1_col3\" class=\"data row1 col3\" >1.7687</td>\n",
       "      <td id=\"T_62657_row1_col4\" class=\"data row1 col4\" >1.8509</td>\n",
       "      <td id=\"T_62657_row1_col5\" class=\"data row1 col5\" >5.8625</td>\n",
       "      <td id=\"T_62657_row1_col6\" class=\"data row1 col6\" >5.3823e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62657_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_62657_row2_col0\" class=\"data row2 col0\" >sigma_50</td>\n",
       "      <td id=\"T_62657_row2_col1\" class=\"data row2 col1\" >41.3323</td>\n",
       "      <td id=\"T_62657_row2_col2\" class=\"data row2 col2\" >0.9949</td>\n",
       "      <td id=\"T_62657_row2_col3\" class=\"data row2 col3\" >2.0575</td>\n",
       "      <td id=\"T_62657_row2_col4\" class=\"data row2 col4\" >2.4169</td>\n",
       "      <td id=\"T_62657_row2_col5\" class=\"data row2 col5\" >7.3456</td>\n",
       "      <td id=\"T_62657_row2_col6\" class=\"data row2 col6\" >9.2037e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62657_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_62657_row3_col0\" class=\"data row3 col0\" >sigma_70</td>\n",
       "      <td id=\"T_62657_row3_col1\" class=\"data row3 col1\" >39.8217</td>\n",
       "      <td id=\"T_62657_row3_col2\" class=\"data row3 col2\" >0.9928</td>\n",
       "      <td id=\"T_62657_row3_col3\" class=\"data row3 col3\" >2.2937</td>\n",
       "      <td id=\"T_62657_row3_col4\" class=\"data row3 col4\" >2.8941</td>\n",
       "      <td id=\"T_62657_row3_col5\" class=\"data row3 col5\" >8.5476</td>\n",
       "      <td id=\"T_62657_row3_col6\" class=\"data row3 col6\" >1.3219e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62657_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_62657_row4_col0\" class=\"data row4 col0\" >sigma_blind</td>\n",
       "      <td id=\"T_62657_row4_col1\" class=\"data row4 col1\" >42.9495</td>\n",
       "      <td id=\"T_62657_row4_col2\" class=\"data row4 col2\" >0.9961</td>\n",
       "      <td id=\"T_62657_row4_col3\" class=\"data row4 col3\" >1.8645</td>\n",
       "      <td id=\"T_62657_row4_col4\" class=\"data row4 col4\" >2.1019</td>\n",
       "      <td id=\"T_62657_row4_col5\" class=\"data row4 col5\" >6.3266</td>\n",
       "      <td id=\"T_62657_row4_col6\" class=\"data row4 col6\" >7.6588e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7e128b4ad5d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test results table to: checkpoints/sert_base_gaussian_test_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import models as _models\n",
    "\n",
    "# Configuration\n",
    "_test_arch = 'sert_base'\n",
    "_test_prefix = 'sert_base_gaussian_test'\n",
    "_resume = True\n",
    "_resume_path = '/kaggle/input/sert-icvl-checkpoint/icvl_gaussian.pth'\n",
    "_sigma_tests = [10,30,50,70, 'blind']  # Add 'blind' as a special case\n",
    "_test_dir_override = '/kaggle/input/icvl-test-512'\n",
    "\n",
    "# Create model\n",
    "print(f\"Creating model {_test_arch}\")\n",
    "_test_net = getattr(_models, _test_arch)()\n",
    "\n",
    "# Device setup\n",
    "if device == 'cuda' and isinstance(gpu_ids, list) and len(gpu_ids) > 1:\n",
    "    _test_net = nn.DataParallel(_test_net.cuda(), device_ids=gpu_ids, output_device=gpu_ids[0])\n",
    "else:\n",
    "    _test_net = _test_net.to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "if _resume and _resume_path and os.path.exists(_resume_path):\n",
    "    print(\"Loading checkpoint from\", _resume_path)\n",
    "    ckpt = torch.load(_resume_path, map_location=device)\n",
    "    state = ckpt.get('net', ckpt)\n",
    "    _test_net.load_state_dict(state, strict=False)\n",
    "else:\n",
    "    print(\"Checkpoint not found or resume disabled, using random weights\")\n",
    "\n",
    "# Handle DataParallel for use_2dconv\n",
    "if isinstance(_test_net, nn.DataParallel):\n",
    "    use_2dconv = getattr(_test_net.module, 'use_2dconv', True)\n",
    "else:\n",
    "    use_2dconv = getattr(_test_net, 'use_2dconv', True)\n",
    "print('use_2dconv =', use_2dconv)\n",
    "\n",
    "# Base dataset\n",
    "_mat_dataset = MatDataFromFolder(_test_dir_override)\n",
    "\n",
    "# Wrapper to run evaluation for a given sigma\n",
    "def run_test_for_sigma(sigma_value):\n",
    "    if sigma_value == 'blind':\n",
    "        noise_adder = AddNoiseBlindv1(10, 70)\n",
    "    else:\n",
    "        noise_adder = lambda img: AddNoise(sigma_value)(img)\n",
    "    \n",
    "    def _mat_transform(sample):\n",
    "        inp, gt = LoadMatHSI(input_key='input', gt_key='gt', needsigma=False)(sample)\n",
    "        inp = inp.numpy() if isinstance(inp, torch.Tensor) else inp\n",
    "        inp_noisy = noise_adder(inp)\n",
    "        return inp_noisy, gt\n",
    "\n",
    "    def _apply_transform(sample):\n",
    "        inp, gt = _mat_transform(sample)\n",
    "        if isinstance(inp, np.ndarray):\n",
    "            inp = torch.from_numpy(inp).float()\n",
    "        if isinstance(gt, np.ndarray):\n",
    "            gt = torch.from_numpy(gt).float()\n",
    "        if not use_2dconv:\n",
    "            inp = inp.unsqueeze(0)  # Add T dimension for 3D model\n",
    "            gt = gt.unsqueeze(0)\n",
    "        return inp, gt\n",
    "\n",
    "    _mat_dataset_wrapped = TransformDatasetWrapper(_mat_dataset, _apply_transform)\n",
    "    _mat_loader = DataLoader(_mat_dataset_wrapped, batch_size=1, shuffle=False, num_workers=1, pin_memory=(device=='cuda'))\n",
    "\n",
    "    print(f\"Testing with noise sigma = {sigma_value}\")\n",
    "    _test_net.eval()\n",
    "    stats = validate_epoch(_test_net, _mat_loader, name=f\"test_sigma{sigma_value}\")\n",
    "    return stats\n",
    "\n",
    "# Run tests for all sigma values\n",
    "_results_rows = []\n",
    "for sigma in _sigma_tests:\n",
    "    sigma_stats = run_test_for_sigma(sigma)\n",
    "    if isinstance(sigma_stats, dict) and len(sigma_stats):\n",
    "        row = {'run': f'sigma_{sigma}'}\n",
    "        row.update({k: v for k, v in sigma_stats.items() if k in ['psnr','ssim','sam','rmse','ergas','loss']})\n",
    "        _results_rows.append(row)\n",
    "\n",
    "# Compile and save results\n",
    "if len(_results_rows) > 0:\n",
    "    results_df = pd.DataFrame(_results_rows)\n",
    "    cols = [c for c in ['run','psnr','ssim','sam','rmse','ergas','loss'] if c in results_df.columns]\n",
    "    results_df = results_df[cols]\n",
    "    display(results_df.style.format({\n",
    "        'psnr': '{:.4f}', 'ssim': '{:.4f}', 'sam': '{:.4f}',\n",
    "        'rmse': '{:.4f}', 'ergas': '{:.4f}', 'loss': '{:.4e}'\n",
    "    }))\n",
    "    out_csv = os.path.join(save_dir, f\"{_test_prefix}_results.csv\")\n",
    "    results_df.to_csv(out_csv, index=False)\n",
    "    print(\"Saved test results table to:\", out_csv)\n",
    "else:\n",
    "    print(\"No test results available\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8391918,
     "sourceId": 13265530,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8406822,
     "sourceId": 13266338,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8410202,
     "sourceId": 13271117,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8411576,
     "sourceId": 13359948,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8391971,
     "sourceId": 13244125,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 464274,
     "modelInstanceId": 447854,
     "sourceId": 598041,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 464274,
     "modelInstanceId": 447854,
     "sourceId": 598618,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 570.605781,
   "end_time": "2025-10-15T11:43:40.572566",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-15T11:34:09.966785",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
